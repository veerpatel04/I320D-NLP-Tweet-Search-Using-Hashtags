{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8LihAbYJs7kP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b05db380-e9f4-4213-a39c-26cb82452d0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install nltk and spacy libraries"
      ],
      "metadata": {
        "id": "OrRDZow9nFBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Search V1.0\n",
        "\n",
        "##Offline Processing"
      ],
      "metadata": {
        "id": "sqdXdR6Utb-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def load_tweets(filename):\n",
        "    with open(filename, 'r') as file:\n",
        "        tweets = file.readlines()\n",
        "    return tweets\n",
        "\n",
        "# Load tweets from file\n",
        "tweets = load_tweets('australian_election_2019_tweets.txt')\n",
        "print(\"Number of tweets before cleaning:\", len(tweets))\n",
        "\n",
        "def remove_duplicates(tweets):\n",
        "    return list(set(tweets))\n",
        "\n",
        "# Remove duplicates\n",
        "tweets = remove_duplicates(tweets)\n",
        "\n",
        "def remove_urls_mentions_hashtags_nonenglish(tweets):\n",
        "    cleaned_tweets = []\n",
        "    for tweet in tweets:\n",
        "        # Remove URLs\n",
        "        tweet = re.sub(r'http\\S+', '', tweet)\n",
        "        # Remove mentions\n",
        "        tweet = re.sub(r'@\\w+', '', tweet)\n",
        "        # Remove hashtags\n",
        "        tweet = re.sub(r'#\\w+', '', tweet)\n",
        "        # Remove non-English text\n",
        "        tweet = re.sub(r'[^\\x00-\\x7F]+', '', tweet)\n",
        "        cleaned_tweets.append(tweet.strip())\n",
        "    return cleaned_tweets\n",
        "\n",
        "# Remove URLs, mentions, hashtags, and non-English text\n",
        "cleaned_tweets = remove_urls_mentions_hashtags_nonenglish(tweets)\n",
        "\n",
        "print(\"Number of tweets after cleaning:\", len(cleaned_tweets))"
      ],
      "metadata": {
        "id": "sS-zkjHKtsFH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dccc54f4-0116-4930-8483-4cb8ce4b3e89"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tweets before cleaning: 347192\n",
            "Number of tweets after cleaning: 264734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After loading the tweets into memory, I ensure data integrity by removing any duplicate entries from the dataset. Next, I focus on cleaning each tweet to refine the dataset further. This involves removing URLs, mentions, hashtags, and any non-English characters present in the text. Leveraging regular expressions, I efficiently identify and eliminate these elements from each tweet. Once the cleaning process is complete, I output the number of tweets remaining after the preprocessing steps. The result is a meticulously curated collection of tweets, stripped of extraneous elements, ready for subsequent analysis or utilization in natural language processing tasks."
      ],
      "metadata": {
        "id": "Ly-kgBGvolZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Real Time Usage"
      ],
      "metadata": {
        "id": "b0AalK3Stj9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def generate_hashtags():\n",
        "    single_word_hashtags = ['#energy', '#laws', '#parliament', '#coalition', '#change']\n",
        "    multiword_hashtags = ['#newPolicy', '#Election2019', '#moreSkill', '#CyberSecurity', '#violenceLaws']\n",
        "\n",
        "    hashtags = single_word_hashtags + multiword_hashtags\n",
        "    hashtags = [hashtag.replace('#', '') for hashtag in hashtags]\n",
        "\n",
        "    return hashtags\n",
        "\n",
        "def string_match_search(tweets, hashtags):\n",
        "    tweet_scores = {hashtag: [] for hashtag in hashtags}\n",
        "\n",
        "    for tweet in tweets:\n",
        "        for hashtag in hashtags:\n",
        "            if hashtag.lower() in tweet.lower():\n",
        "                score = sum(1 for word in tweet.split() if word.lower() == hashtag.lower())\n",
        "                tweet_scores[hashtag].append((tweet, score))\n",
        "\n",
        "    top_tweets = {}\n",
        "    for hashtag, scores in tweet_scores.items():\n",
        "        sorted_tweets = sorted(scores, key=lambda x: x[1], reverse=True)[:5]\n",
        "        top_tweets[hashtag] = [tweet for tweet, _ in sorted_tweets]\n",
        "\n",
        "    return top_tweets\n",
        "\n",
        "# Generate hashtags\n",
        "hashtags = generate_hashtags()\n",
        "\n",
        "# Perform string match-based search\n",
        "top_tweets = string_match_search(cleaned_tweets, hashtags)\n",
        "\n",
        "# Display top 5 tweets for each hashtag\n",
        "for hashtag, tweets in top_tweets.items():\n",
        "    print(f'Top 5 tweets for {hashtag}:')\n",
        "    for i, tweet in enumerate(tweets, 1):\n",
        "        print(f'{i}. {tweet}')\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAi4L8iZtjTV",
        "outputId": "dfd7b0dd-358e-4e85-e74a-4efffa6d76e9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 tweets for energy:\n",
            "1. CO2 is NOT a pollutant!  It is an essential nutrient for plants; in the process of photosynthesis, plants convert radiant energy from sun into chemical energy in the form of glucose or sugar:  6 H2O + 6CO2 + radiant energy (sunlight) --&gt; C6H12O6 (glucose) + 6O2 (oxygen)\n",
            "2. As we head into The Climate election ponder the possible future of free energy and Australia as a renewable superpower. Its on its way. The radical path to free energy for all - Financial Review, 5/17/2019\n",
            "3. Yet we havent seen one media outlet cover Michael Wests investigation of the incestuous relationships b/w LNP &amp; the Coal Industry    Liberal partys rank opportunism spells danger for Australian energy policy | Energy | The Guardian\n",
            "4. Barnaby Joyce spruiking energy policy is important on TV with cheering constituents behind him and his party doesn't have an energy policy.\n",
            "5. Low Energy Ollie already coming up with excuses for a low energy result tonight. Sad!\n",
            "\n",
            "Top 5 tweets for laws:\n",
            "1. Most states have laws to disband bikkie gangs. Those laws are for hate, violence etc. There are similarities btw bikkies and the hatred and vile greens. We need similar laws against the greens.\n",
            "2. SCOMO cooked the  menzies dildo  and  now its time  juicey cock fuck   deliberately breached Australia environmental laws to approve these 2 controversial mines on the eve of    no respect for our laws\n",
            "3. SCOMO cooked the  menzies dildo  and  now its time  juicey cock fuck   deliberately breached Australia environmental laws to approve these 2 controversial mines on the eve of    no respect for our laws\n",
            "4. But it does matter that a politician have policies that obey the basic laws of economics (or at least not defy basic laws and research). Because one day\n",
            "5. 4. Media overhaul including truth in media laws and ownership laws\n",
            "\n",
            "Top 5 tweets for parliament:\n",
            "1. Misleading of parliament by knowingly presenting false info to Parliament is a very serious charge in the Westminster system. So why is it somehow acceptable to knowingly mislead voters as a means of getting into Parliament in the 1st place? We demand the lies stop.\n",
            "2. 62 scientists &amp; experts signed open letter to the next Parliament of Australia, calling for victorious Government this Saturday to make urgent action on climate change a top priority for the 46th Parliament of Australia.\n",
            "3. Reminds me of the time when . was the No Pokies candidate and won a seat in state parliament then rose to federal status. I hope he returns to local parliament one day.\n",
            "4. You shut down Parliament to avoid losing a vote on the floor of the house and had parliament sit for only 8 days in half a year!\n",
            "5. Former Chief Scientist for Australia Penny Sackett and  Climate &amp; Energy Director , front the cameras at Parliament House: 62 experts urge next parliament to make climate action a top priority\n",
            "\n",
            "Top 5 tweets for coalition:\n",
            "1. Yeah, to be fair the swing towards the coalition was bigger in NSW and much bigger in SA. The primary vote for the coalition was bigger in WA than Qld. The primary vote was bigger for the coalition than Labor in every state. Labor only won NT and ACT. Australia decided not Qld.\n",
            "2. Australia election: Liberal-led coalition headed for surprise win: Opposition leader concedes defeat after Liberal-National coalition performs better than expected in general election.\n",
            "3. Handing power to the men. BBC News: 2019 Australia election: Morrison's coalition seeking shock majority BBC News - 2019 Australia election: Morrison's coalition seeking shock majority\n",
            "4. Scott Morrison has led the Coalition to victory in this years federal election. Bill Shorten has said he will he stand down as Labor leader and at this stage, its not clear whether the Coalition will lead as a majority or minority government.\n",
            "5. \"The types of project each party is promising reflect a pattern seen in the VIC and NSW elections. The Coalition will outspend Labor on roads; Labor will outspend the Coalition on public transport.\n",
            "\n",
            "Top 5 tweets for change:\n",
            "1. If we have a change in government on Saturday - Australe will once again be a world leader on climate change - How Australias election will decide its role in climate change\n",
            "2. Bill Shorten always thinks it's \"time to change\". It was time to change to Rudd, it was time to change to Gillard and then it was time to change back to Rudd\n",
            "3. Shorten: Never has your vote been more important. Never has the case for change been more clear or more urgent. Because just as Blacktown tells us the story of the change that Australia voted for back then, it also speaks to why our country should vote for change now.\n",
            "4. Australia's election this year will be about change. Change happens whether we want it or not.  The issue is not so much what might be changed from the current regime, but how change is managed. One thing is certain - conservatives take note - change cannot be denied.\n",
            "5. Bills on about climate change deniers- alarmists have denied it - he is correct since  the satellite data contradicted the change proposed by the climate council and others, and showed change was around 1.6deg/100yrs as established earlier,\n",
            "\n",
            "Top 5 tweets for newPolicy:\n",
            "\n",
            "Top 5 tweets for Election2019:\n",
            "1. John Lords Election Diary No. 16: Final thoughts but first lets talk about Bob   Election2019    via\n",
            "2. Peter Pilts Musings The Afternoon After Australia Decides  Election2019\n",
            "3. Turnbull robocall urges voters to dump conservative Liberals | The New Daily  Eelection2019\n",
            "4. The rant iv heard most this election is  are going to send  bankrupt. Voters need a simple table of  &amp; . Where is the Money coming from?  Election2019: Labor vs Liberal on the economy\n",
            "5. Election2019,\n",
            "\n",
            "Top 5 tweets for moreSkill:\n",
            "\n",
            "Top 5 tweets for CyberSecurity:\n",
            "1. After the Parliament cybersecurity breach, MPs and senators should be looking closer to home  Hmmmn\n",
            "2. Bloomberg Shits the Bed Again on Cybersecurity meanwhile in Australia the govt want business to install backdoors in products   First class idiots\n",
            "3. BS was his own worst enemy.  Consider an employee who would advocate breaking cybersecurity and then bitching about email security failure.  BS was a fuckwit.\n",
            "4. Ed Husic on Cybersecurity: We dont have enough people in Australia with cyber skills. Its not sustainable. We want to make investments to make sure TAFE can meet these needs but we need more of a long-term focus. We recognise we need to do more.\n",
            "\n",
            "Top 5 tweets for violenceLaws:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function generate_hashtags() is used to create a list of hashtags, combining single-word and multiword hashtags while removing the '#' symbol. Then, the string_match_search(tweets, hashtags) function is implemented to find relevant tweets for each hashtag. It iterates through tweets and hashtags, calculating relevance scores based on hashtag occurrences in tweets. Top 5 tweets for each hashtag are extracted and stored in a dictionary. Finally, I generate hashtags, perform the search, and display the top 5 tweets for each hashtag. This process offers insights into the most pertinent tweets for each topic."
      ],
      "metadata": {
        "id": "kMffPh_8o8h-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Search V2.0\n",
        "\n",
        "##Offline Processing"
      ],
      "metadata": {
        "id": "LCkqbJLl0V97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def load_tweets(filename):\n",
        "    with open(filename, 'r') as file:\n",
        "        tweets = file.readlines()\n",
        "    return tweets\n",
        "\n",
        "# Load tweets from file\n",
        "tweets = load_tweets('australian_election_2019_tweets.txt')\n",
        "print(\"Number of tweets before cleaning:\", len(tweets))\n",
        "\n",
        "def remove_duplicates(tweets):\n",
        "    return list(set(tweets))\n",
        "\n",
        "# Remove duplicates\n",
        "tweets = remove_duplicates(tweets)\n",
        "\n",
        "def remove_urls_mentions_hashtags_nonenglish(tweets):\n",
        "    cleaned_tweets = []\n",
        "    for tweet in tweets:\n",
        "        # Remove URLs\n",
        "        tweet = re.sub(r'http\\S+', '', tweet)\n",
        "        # Remove mentions\n",
        "        tweet = re.sub(r'@\\w+', '', tweet)\n",
        "        # Remove hashtags\n",
        "        tweet = re.sub(r'#\\w+', '', tweet)\n",
        "        # Remove non-English text\n",
        "        tweet = re.sub(r'[^\\x00-\\x7F]+', '', tweet)\n",
        "        cleaned_tweets.append(tweet.strip())\n",
        "    return cleaned_tweets\n",
        "\n",
        "# Remove URLs, mentions, hashtags, and non-English text\n",
        "cleaned_tweets = remove_urls_mentions_hashtags_nonenglish(tweets)\n",
        "\n",
        "print(\"Number of tweets after cleaning:\", len(cleaned_tweets))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyXUldvH0UDZ",
        "outputId": "f02c22dd-cd0a-4e01-cf7f-d95edbbba0cc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tweets before cleaning: 347192\n",
            "Number of tweets after cleaning: 264734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the same code as sued in the Search V1.0."
      ],
      "metadata": {
        "id": "J8A4AO-PpRDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing Techniques"
      ],
      "metadata": {
        "id": "cTU26QbFpX5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess_tweet(tweet):\n",
        "    # Lower casing\n",
        "    tweet = tweet.lower()\n",
        "    # Tokenization\n",
        "    words = word_tokenize(tweet)\n",
        "    # Stemming\n",
        "    ps = PorterStemmer()\n",
        "    stemmed_words = [ps.stem(word) for word in words]\n",
        "    # Stopword removal\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in stemmed_words if word not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "def preprocess_tweets(tweets):\n",
        "    preprocessed_tweets = [preprocess_tweet(tweet) for tweet in tweets]\n",
        "    return preprocessed_tweets\n",
        "\n",
        "# Apply text pre-processing techniques\n",
        "preprocessed_tweets = preprocess_tweets(cleaned_tweets)\n",
        "\n",
        "print(\"Number of tweets after cleaning and preprocessing:\", len(preprocessed_tweets))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdO7csyZAE0-",
        "outputId": "85a890dd-6689-4e2d-a0fb-fac4c7a5aca0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tweets after cleaning and preprocessing: 264734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is for preprocessing text data using regular expressions and NLTK. It employs lowercasing, tokenization, stemming, and stopword removal for cleaning tweets. The preprocess_tweet(tweet) function handles individual tweet preprocessing, while the preprocess_tweets(tweets) function extends this cleaning to a list of tweets. Finally, the script prints the number of tweets after preprocessing, demonstrating its effectiveness in enhancing data quality."
      ],
      "metadata": {
        "id": "MTUpMt7UrM4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# B. Real Time Usage\n",
        "def string_match_search_v2(tweets, hashtags):\n",
        "    tweet_scores = {hashtag: [] for hashtag in hashtags}\n",
        "\n",
        "    for tweet in tweets:\n",
        "        preprocessed_tweet = preprocess_tweet(tweet)\n",
        "        for hashtag in hashtags:\n",
        "            if hashtag.lower() in preprocessed_tweet:\n",
        "                score = sum(1 for word in preprocessed_tweet.split() if word.lower() == hashtag.lower())\n",
        "                tweet_scores[hashtag].append((tweet, score))\n",
        "\n",
        "    top_tweets = {}\n",
        "    for hashtag, scores in tweet_scores.items():\n",
        "        sorted_tweets = sorted(scores, key=lambda x: x[1], reverse=True)[:5]\n",
        "        top_tweets[hashtag] = [tweet for tweet, _ in sorted_tweets]\n",
        "\n",
        "    return top_tweets\n",
        "\n",
        "# Perform search for the same set of hashtags\n",
        "top_tweets_v2 = string_match_search_v2(cleaned_tweets, hashtags)\n",
        "\n",
        "# Display top 5 tweets for each hashtag after preprocessing\n",
        "for hashtag, tweets in top_tweets_v2.items():\n",
        "    print(f'Top 5 tweets for {hashtag}:')\n",
        "    for i, tweet in enumerate(tweets, 1):\n",
        "        print(f'{i}. {tweet}')\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jrasbzlrbRv",
        "outputId": "3f53676a-ed81-4f7e-80d2-bb5fd05c7e8d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 tweets for energy:\n",
            "1. Wht ws it if not FASCISM/TREASN tht NEO-COMMUNIST GRNLAB/TBULL/LEFT 1st weaknd COAL-our only reliab energy-by scaring off ALL INVESTRS &amp; BANKS W/WIDE-made it a PARIAH-STOLE GOVT by COUP 2 enslave COAL 2 th LW COUPGOVT/BIZ-favord uselss INTRMITTNTS SO 'RE' CAN DISABLE A'a\n",
            "2. ETEnergyworld | COLUMN-Australia's shock election shows killing coal mining is no sure thing: Russell\n",
            "3. Australia cant afford the climate-denying, energy-policy-free, unstable LNP.  Vote in thinking candidates unshackled by the orthodoxies of the major parties.\n",
            "4. Wht is it if not FASCISM/TREASN tht NEO-COMMUNIST GRNLAB/TBULL/LEFT 1st weakns COAL-our only reliab energy-by scaring off ALL INVESTRS &amp; BANKS W/WIDE-makes it a PARIAH-STEALS GOVT by COUP 2 nslave COAL 2 th LW COUPGOVT/BIZ-favord uselss INTRMITTNTS SO 'RE' CN DISABLE A'a\n",
            "5. Labor is ready to act on climate change by reducing Australias pollution by 45 per cent on 2005 levels by 2030 and reaching net zero pollution by 2050. We will also deliver 50 per cent renewable energyby 2030. Vote Labor this week!\n",
            "\n",
            "Top 5 tweets for laws:\n",
            "1. This is why Capilano Honey are running their frivolous &amp; vexatious lawsuit against me &amp; Simon Mulvany. They are worried about being sued for the weedkiller Roundup in their honey:\n",
            "2. Vote  for the long promised \"the great Republic of the Southern Seas\" (Louisa Lawson 1887).  Either Labor and Greens 1/2 or 2/1 - or reliable YES minors and independents. Onwards republicans!\n",
            "3. Calling all   has been hit with a defamation lawsuit by an  politician. We need to set up a GoFundMe for him. Comedians, Journalist, and people who critique Politicians should not be sued for defamation.\n",
            "4. Not only is  going under, now the  Indiginous people have filed a UN lawsuit.\n",
            "5. UK &amp; US experiences show us that electoral laws/rules must be enforced both *fast* and *hard*.\n",
            "\n",
            "Top 5 tweets for parliament:\n",
            "1. Misleading of parliament by knowingly presenting false info to Parliament is a very serious charge in the Westminster system. So why is it somehow acceptable to knowingly mislead voters as a means of getting into Parliament in the 1st place? We demand the lies stop.\n",
            "2. The Westminster model is where parties get their mandate through parliament. In other words people vote for a party to get a seat in parliament. Then parliament decides who is president.\n",
            "3. Electorate = geographically defined areas Australians live in. They are represented in parliament by a single elected Member of Parliament. MPs go to parliament to debate ideas and policy, acting for you and people in your community\n",
            "4. 62 scientists &amp; experts signed open letter to the next Parliament of Australia, calling for victorious Government this Saturday to make urgent action on climate change a top priority for the 46th Parliament of Australia.\n",
            "5. Reminds me of the time when . was the No Pokies candidate and won a seat in state parliament then rose to federal status. I hope he returns to local parliament one day.\n",
            "\n",
            "Top 5 tweets for coalition:\n",
            "1. That was a very good showing from  on  this morning. He could be a very good PM ... has vision and he gets a huge  from me for keeping a major political party united for years. Something the Coalition/Greens cant do. Govern, dont politick\n",
            "2. Coalition- 66 seats\n",
            "3. Senate Projection by State (Coalition-Labor-Greens-Other)\n",
            "4. corrupt Coalition- cheating right to the end. Took alot of planning too. This was not a spontaneous single action cheat! Shame on them. OZ democracy better than this\n",
            "5. Labor has a much better climate change policy than the coalition.- Tony Abbott\n",
            "\n",
            "Top 5 tweets for change:\n",
            "1. One of the things that makes me proud to be Australian is the effort our electoral commission goes to in order to make sure everyone gets the chance to vote. It is a game changer compared to nearly every other country.\n",
            "2. We have to focus on how to get Labor over the line in 3years&amp; encourage changemakers.\n",
            "3. WAKE up NZ! \"climate change crisis\" is NOT a \"thing\". Its NOT valid. Its NOT a crisis. Climate ALWAYS changes=completely NATURAL. NZ needs to STOP taxing its citizens for NON EXISTENT \"problem\"!\n",
            "4. Don't come crying to us when you LNP/ON/UAP/Katter Party-voting c/change-denying arseholes need taxpayer support for the ever-increasing floods/droughts/cyclones/hailstorms etc you refuse to accept is caused by c/change.\n",
            "5. Labor's announcement is a game changer for workers and job security...the rolling fixed term contract is a rort.\n",
            "\n",
            "Top 5 tweets for newPolicy:\n",
            "\n",
            "Top 5 tweets for Election2019:\n",
            "1. John Lords Election Diary No. 16: Final thoughts but first lets talk about Bob   Election2019    via\n",
            "2. The rant iv heard most this election is  are going to send  bankrupt. Voters need a simple table of  &amp; . Where is the Money coming from?  Election2019: Labor vs Liberal on the economy\n",
            "3. Election2019,\n",
            "4. Peter Pilts Musings The Afternoon After Australia Decides  Election2019\n",
            "5. Turnbull robocall urges voters to dump conservative Liberals | The New Daily  Eelection2019\n",
            "\n",
            "Top 5 tweets for moreSkill:\n",
            "\n",
            "Top 5 tweets for CyberSecurity:\n",
            "\n",
            "Top 5 tweets for violenceLaws:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the same code that was used in Search V1.0 for real time usage. It has been adapted for real time usage in this scenario."
      ],
      "metadata": {
        "id": "lVBGFNGsreYQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Search Results and Insights:\n",
        "\n",
        "##1. Simple Word-Overlap Based Match (Search V1.0):\n",
        "\n",
        "###Offline Processing:\n",
        "In this phase, the script loads tweets from the file and performs initial data cleaning tasks, including removing duplicates and eliminating extraneous elements such as URLs, mentions, hashtags, and non-English text. This ensures that the dataset is streamlined and ready for further analysis. Removing duplicates helps in reducing redundancy, while cleaning ensures that only relevant text is retained for subsequent processing.\n",
        "\n",
        "###Real Time Usage:\n",
        "Once the tweets are preprocessed, the script generates hashtags and conducts a string match-based search to extract the top 5 tweets for each hashtag. The relevance of tweets is determined based on word occurrences in the hashtags. Analyzing these top tweets provides valuable insights into the most discussed topics on Twitter, enabling a deeper understanding of public opinions, trends, and sentiments surrounding various hashtags.\n",
        "\n",
        "\n",
        "##2. Improving Search Quality with Text-preprocessing (Search V2.0):\n",
        "\n",
        "###Offline Processing:\n",
        "In this stage, the script enhances the search quality by applying advanced text preprocessing techniques to the tweets. This includes lower casing, tokenization, stemming, and stopword removal. Lower casing ensures uniformity, tokenization breaks text into meaningful units, stemming reduces words to their root form, and stopword removal eliminates common words that do not contribute to the meaning. These preprocessing steps help in standardizing the text and removing noise, thereby improving the quality of the dataset.\n",
        "\n",
        "###Real Time Usage:\n",
        "Building upon the preprocessed data, the script conducts the same string match-based search as in V1.0 but on the cleaned tweets. Relevance scores are still determined based on word occurrences in the hashtags. However, the preprocessing steps applied in V2.0 lead to more accurate and meaningful search results compared to V1.0. By removing noise and standardizing the text, the search algorithm can better identify and extract tweets that are truly relevant to the hashtags, resulting in higher-quality insights and analysis.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lUsq7cnTtyu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used some online resources to help me with my code. These include:\n",
        "\n",
        "https://stackoverflow.com/questions/64719706/cleaning-twitter-data-pandas-python\n",
        "\n",
        "https://aronakhmad.medium.com/twitter-data-cleaning-using-python-db1ec2f28f08\n",
        "\n",
        "https://stackoverflow.com/questions/51717995/extracting-data-tweets-based-on-a-specific-hashtag\n",
        "\n",
        "https://github.com/shresth26/Twitter-Hashtag-Analysis"
      ],
      "metadata": {
        "id": "JaEyHHdaAW4d"
      }
    }
  ]
}